#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞—É–¥–∏—Ç–∞ —Å–æ–≥–ª–∞—Å–Ω–æ CURSOR_AUDIT_BUNDLE_TASK.md
"""

import json
import hashlib
import os
from pathlib import Path
from datetime import datetime
import pandas as pd
import yaml

def calculate_md5(file_path):
    """–í—ã—á–∏—Å–ª—è–µ—Ç MD5 —Ö–µ—à —Ñ–∞–π–ª–∞"""
    hash_md5 = hashlib.md5()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()

def redact_secrets(content, keys_to_redact):
    """–£–¥–∞–ª—è–µ—Ç —Å–µ–∫—Ä–µ—Ç—ã –ø–µ—Ä–µ–¥ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ–º —Ö–µ—à–∞"""
    for key in keys_to_redact:
        # –ü—Ä–æ—Å—Ç–∞—è –∑–∞–º–µ–Ω–∞ –∫–ª—é—á–µ–π –≤ —Ç–µ–∫—Å—Ç–µ
        import re
        patterns = [
            rf'{key}:\s*["\']?[^"\'\s]+["\']?',
            rf'"{key}":\s*["\']?[^"\'\s]+["\']?',
            rf'{key}\s*=\s*["\']?[^"\'\s]+["\']?',
        ]
        for pattern in patterns:
            content = re.sub(pattern, f'{key}: ***', content, flags=re.IGNORECASE)
    return content

def analyze_file(file_path, required_columns=None):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"""
    file_path = Path(file_path)
    if not file_path.exists():
        return {"status": "missing", "error": "File not found"}
    
    try:
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞
        stat = file_path.stat()
        file_size = stat.st_size
        
        if file_size == 0:
            return {"status": "missing", "error": "Zero-byte file"}
        
        # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–ª—è —Ö–µ—à–∞ (—Å —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Å–µ–∫—Ä–µ—Ç–æ–≤)
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        # –†–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º —Å–µ–∫—Ä–µ—Ç—ã –ø–µ—Ä–µ–¥ —Ö–µ—à–µ–º
        secrets = ['api_key', 'secret', 'passphrase', 'private_key']
        content_redacted = redact_secrets(content, secrets)
        md5_hash = hashlib.md5(content_redacted.encode('utf-8')).hexdigest()
        md5_hash_original = hashlib.md5(content.encode('utf-8')).hexdigest()
        
        # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å
        try:
            rel_path = str(file_path.relative_to(Path.cwd()))
        except ValueError:
            # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Ç—å –∫–∞–∫ –µ—Å—Ç—å
            rel_path = str(file_path).replace(str(Path.cwd()), "").lstrip("\\/")
        
        result = {
            "status": "ok",
            "file_path": rel_path,
            "absolute_path": str(file_path.absolute()),
            "file_size_bytes": file_size,
            "date_modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
            "md5_hash": md5_hash,
            "md5_hash_original": md5_hash_original,
        }
        
        # –ï—Å–ª–∏ —ç—Ç–æ CSV –∏–ª–∏ –º–æ–∂–Ω–æ –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫–∞–∫ —Ç–∞–±–ª–∏—Ü—É
        if file_path.suffix.lower() in ['.csv', '.parquet']:
            try:
                if file_path.suffix.lower() == '.csv':
                    try:
                        df = pd.read_csv(file_path, encoding='utf-8')
                    except UnicodeDecodeError:
                        # –ü—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
                        for enc in ['utf-8-sig', 'cp1251', 'koi8-r']:
                            try:
                                df = pd.read_csv(file_path, encoding=enc)
                                break
                            except:
                                continue
                        else:
                            raise
                else:
                    df = pd.read_parquet(file_path)
                
                result["shape"] = {"rows": len(df), "columns": len(df.columns)}
                result["columns"] = list(df.columns)
                
                # –°–ª–æ–≤–∞—Ä—å –∫–æ–ª–æ–Ω–æ–∫
                columns_dict = {}
                for col in df.columns:
                    col_type = str(df[col].dtype)
                    columns_dict[col] = {
                        "type": col_type,
                        "description": f"Column {col}",
                        "sample_values": df[col].head(3).tolist() if len(df) > 0 else []
                    }
                result["columns_dict"] = columns_dict
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
                if required_columns:
                    missing = [col for col in required_columns if col not in df.columns]
                    if missing:
                        result["status"] = "error"
                        result["error"] = f"Missing required columns: {missing}"
                        result["missing_columns"] = missing
                
            except Exception as e:
                result["status"] = "error"
                result["error"] = f"Failed to read as table: {str(e)}"
        
        return result
        
    except Exception as e:
        return {"status": "error", "error": str(e)}

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞"""
    print("üìã –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–• –î–õ–Ø –ê–£–î–ò–¢–ê")
    print("=" * 60)
    
    bundle = {
        "metadata": {
            "created_at": datetime.now().isoformat(),
            "strategy_name": "futures_scalping",
            "version": "1.2"
        },
        "files": {},
        "integrity_errors": []
    }
    
    # 1. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    print("\n1. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã...")
    config_files = [
        "config/config_futures.yaml",
        "config.yaml"
    ]
    
    for config_file in config_files:
        if Path(config_file).exists():
            print(f"   ‚úÖ {config_file}")
            bundle["files"]["config"] = analyze_file(config_file)
            break
    
    # 2. Strategy —Ñ–∞–π–ª
    print("\n2. –§–∞–π–ª —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏...")
    strategy_files = [
        "src/strategies/scalping/futures/orchestrator.py",
        "src/strategies/scalping/futures/signal_generator.py"
    ]
    
    for strategy_file in strategy_files:
        if Path(strategy_file).exists():
            print(f"   ‚úÖ {strategy_file}")
            bundle["files"]["strategy"] = analyze_file(strategy_file)
            break
    
    # 3. Trades CSV
    print("\n3. –§–∞–π–ª—ã —Å–¥–µ–ª–æ–∫...")
    target_folder = Path("logs/futures/archived/logs_2025-12-07_16-03-39")
    trades_file = target_folder / "trades_2025-12-07.csv"
    
    if trades_file.exists():
        print(f"   ‚úÖ {trades_file}")
        required_columns = [
            "trade_id", "timestamp_open", "timestamp_close", 
            "side", "qty", "price_open", "price_close", 
            "fee", "symbol"
        ]
        bundle["files"]["trades"] = analyze_file(trades_file, required_columns)
    else:
        print(f"   ‚ö†Ô∏è {trades_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        bundle["integrity_errors"].append({
            "file": "trades_2025-12-07.csv",
            "error": "File not found",
            "severity": "critical"
        })
    
    # 4. Orders CSV
    print("\n4. –§–∞–π–ª—ã –æ—Ä–¥–µ—Ä–æ–≤...")
    orders_files = list(Path("logs/futures").glob("orders_*.csv"))
    if orders_files:
        latest_orders = max(orders_files, key=lambda p: p.stat().st_mtime)
        print(f"   ‚úÖ {latest_orders}")
        required_columns = [
            "order_id", "timestamp_submit", "timestamp_fill",
            "side", "type", "qty", "price", "status"
        ]
        bundle["files"]["orders"] = analyze_file(latest_orders, required_columns)
    else:
        print("   ‚ö†Ô∏è orders_*.csv –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    # 5. Positions Open CSV
    print("\n5. –§–∞–π–ª—ã –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ø–æ–∑–∏—Ü–∏–π...")
    positions_files = list(Path("logs/futures").glob("positions_open_*.csv"))
    if positions_files:
        latest_positions = max(positions_files, key=lambda p: p.stat().st_mtime)
        print(f"   ‚úÖ {latest_positions}")
        bundle["files"]["positions_open"] = analyze_file(latest_positions)
    else:
        print("   ‚ö†Ô∏è positions_open_*.csv –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    # 6. Signals CSV
    print("\n6. –§–∞–π–ª—ã —Å–∏–≥–Ω–∞–ª–æ–≤...")
    signals_files = list(Path("logs/futures").glob("signals_*.csv"))
    if signals_files:
        latest_signals = max(signals_files, key=lambda p: p.stat().st_mtime)
        print(f"   ‚úÖ {latest_signals}")
        bundle["files"]["signals"] = analyze_file(latest_signals)
    else:
        print("   ‚ö†Ô∏è signals_*.csv –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    output_file = f"audit_bundle_{datetime.now().strftime('%Y%m%d')}_futures_scalping.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(bundle, f, indent=2, ensure_ascii=False)
    
    print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_file}")
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   –§–∞–π–ª–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len([f for f in bundle['files'].values() if f.get('status') == 'ok'])}")
    print(f"   –û—à–∏–±–æ–∫: {len(bundle['integrity_errors'])}")
    
    return bundle

if __name__ == "__main__":
    main()

