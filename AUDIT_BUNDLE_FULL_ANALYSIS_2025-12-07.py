#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð· Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð°ÑƒÐ´Ð¸Ñ‚Ð° ÑÐ¾Ð³Ð»Ð°ÑÐ½Ð¾ CURSOR_AUDIT_BUNDLE_TASK.md v1.2
"""

import json
import hashlib
import os
import sys
import re
from pathlib import Path
from datetime import datetime
import pandas as pd
import yaml
import platform

def calculate_md5(file_path):
    """Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÑ‚ MD5 Ñ…ÐµÑˆ Ñ„Ð°Ð¹Ð»Ð°"""
    hash_md5 = hashlib.md5()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()

def redact_secrets(content, keys_to_redact):
    """Ð£Ð´Ð°Ð»ÑÐµÑ‚ ÑÐµÐºÑ€ÐµÑ‚Ñ‹ Ð¿ÐµÑ€ÐµÐ´ Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸ÐµÐ¼ Ñ…ÐµÑˆÐ°"""
    for key in keys_to_redact:
        patterns = [
            rf'{key}:\s*["\']?[^"\'\s]+["\']?',
            rf'"{key}":\s*["\']?[^"\'\s]+["\']?',
            rf'{key}\s*=\s*["\']?[^"\'\s]+["\']?',
        ]
        for pattern in patterns:
            content = re.sub(pattern, f'{key}: ***', content, flags=re.IGNORECASE)
    return content

def analyze_trades_csv(file_path):
    """ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ trades.csv Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¾Ð¹ Ñ†ÐµÐ»Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸"""
    file_path = Path(file_path)
    result = {
        "file_path": str(file_path),
        "status": "ok"
    }
    
    try:
        # Ð§Ð¸Ñ‚Ð°ÐµÐ¼ CSV
        df = pd.read_csv(file_path)
        result["shape"] = {"rows": len(df), "columns": len(df.columns)}
        result["columns"] = list(df.columns)
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº (Ð¼Ð°Ð¿Ð¿Ð¸Ð½Ð³ Ð½Ð°ÑˆÐ¸Ñ… ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº Ð½Ð° Ñ‚Ñ€ÐµÐ±ÑƒÐµÐ¼Ñ‹Ðµ)
        column_mapping = {
            "timestamp": "timestamp_close",
            "entry_price": "price_open",
            "exit_price": "price_close",
            "size": "qty",
            "commission": "fee"
        }
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð²
        if "timestamp" in df.columns and "symbol" in df.columns:
            duplicates = df[df.duplicated(subset=["timestamp", "symbol"], keep=False)]
            if len(duplicates) > 0:
                result["integrity_errors"] = result.get("integrity_errors", {})
                result["integrity_errors"]["duplicates"] = {
                    "count": len(duplicates),
                    "lines": duplicates.index.tolist()
                }
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ¾Ð²
        missing_values = {}
        for col in df.columns:
            null_count = df[col].isnull().sum()
            if null_count > 0:
                missing_values[col] = {
                    "count": int(null_count),
                    "lines": df[df[col].isnull()].index.tolist()
                }
        if missing_values:
            result["integrity_errors"] = result.get("integrity_errors", {})
            result["integrity_errors"]["missing_values"] = missing_values
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð·Ð½Ð°ÐºÐ¾Ð² qty/size
        if "side" in df.columns and "size" in df.columns:
            violations = []
            for idx, row in df.iterrows():
                side = str(row["side"]).lower()
                size = float(row["size"]) if pd.notna(row["size"]) else 0
                if side == "long" and size < 0:
                    violations.append({"line": int(idx), "side": side, "size": size})
                elif side == "short" and size > 0:
                    violations.append({"line": int(idx), "side": side, "size": size})
            if violations:
                result["integrity_errors"] = result.get("integrity_errors", {})
                result["integrity_errors"]["qty_signs"] = {"violations": violations}
        
        # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°
        result["total_trades"] = len(df)
        if "net_pnl" in df.columns:
            winning = df[df["net_pnl"] > 0]
            losing = df[df["net_pnl"] < 0]
            result["winning_trades"] = len(winning)
            result["losing_trades"] = len(losing)
            result["total_pnl"] = float(df["net_pnl"].sum())
        
        if "symbol" in df.columns:
            result["symbols"] = df["symbol"].unique().tolist()
        
        if "timestamp" in df.columns:
            result["date_range"] = {
                "start": str(df["timestamp"].min()),
                "end": str(df["timestamp"].max())
            }
        
        return result
        
    except Exception as e:
        result["status"] = "error"
        result["error"] = str(e)
        return result

def main():
    """ÐžÑÐ½Ð¾Ð²Ð½Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°"""
    print("=" * 70)
    print("ðŸ“‹ ÐŸÐžÐ›ÐÐ«Ð™ ÐÐÐÐ›Ð˜Ð— Ð”ÐÐÐÐ«Ð¥ Ð”Ð›Ð¯ ÐÐ£Ð”Ð˜Ð¢Ð")
    print("   Ð¡Ð¾Ð³Ð»Ð°ÑÐ½Ð¾ CURSOR_AUDIT_BUNDLE_TASK.md v1.2")
    print("=" * 70)
    
    bundle = {
        "metadata": {
            "bundle_id": f"audit_bundle_{datetime.now().strftime('%Y%m%d')}_futures_scalping",
            "created_at": datetime.now().isoformat(),
            "created_by": "Cursor AI",
            "strategy_name": "futures_scalping",
            "strategy_version": "1.0",
            "project_root": str(Path.cwd())
        },
        "environment": {
            "python_version": sys.version.split()[0],
            "os": platform.system() + " " + platform.release(),
            "platform": platform.platform()
        },
        "timezone": {
            "exchange": "UTC",
            "strategy": "UTC",
            "note": "All timestamps in logs are UTC"
        },
        "trading_mode": {
            "type": "live",
            "sandbox": True,
            "description": "Live trading on OKX sandbox (demo account)"
        },
        "files": {},
        "integrity_errors": []
    }
    
    # 1. ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ
    print("\n1ï¸âƒ£ ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ð¾Ð½Ð½Ñ‹Ðµ Ñ„Ð°Ð¹Ð»Ñ‹...")
    config_file = Path("config/config_futures.yaml")
    if config_file.exists():
        print(f"   âœ… {config_file}")
        stat = config_file.stat()
        with open(config_file, 'r', encoding='utf-8') as f:
            content = f.read()
        content_redacted = redact_secrets(content, ['api_key', 'secret', 'passphrase'])
        bundle["files"]["config"] = {
            "status": "ok",
            "file_path": str(config_file),
            "file_size_bytes": stat.st_size,
            "date_modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
            "md5_hash": hashlib.md5(content_redacted.encode()).hexdigest(),
            "md5_hash_original": hashlib.md5(content.encode()).hexdigest()
        }
    else:
        print(f"   âš ï¸ {config_file} Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½")
        bundle["integrity_errors"].append({"file": "config", "error": "File not found"})
    
    # 2. Strategy Ñ„Ð°Ð¹Ð»
    print("\n2ï¸âƒ£ Ð¤Ð°Ð¹Ð» ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¸...")
    strategy_file = Path("src/strategies/scalping/futures/orchestrator.py")
    if strategy_file.exists():
        print(f"   âœ… {strategy_file}")
        stat = strategy_file.stat()
        bundle["files"]["strategy"] = {
            "status": "ok",
            "file_path": str(strategy_file),
            "file_size_bytes": stat.st_size,
            "date_modified": datetime.fromtimestamp(stat.st_mtime).isoformat()
        }
    else:
        print(f"   âš ï¸ {strategy_file} Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½")
    
    # 3. Trades CSV
    print("\n3ï¸âƒ£ Ð¤Ð°Ð¹Ð»Ñ‹ ÑÐ´ÐµÐ»Ð¾Ðº...")
    trades_file = Path("logs/futures/archived/logs_2025-12-07_16-03-39/trades_2025-12-07.csv")
    if trades_file.exists():
        print(f"   âœ… {trades_file}")
        stat = trades_file.stat()
        analysis = analyze_trades_csv(trades_file)
        analysis["file_size_bytes"] = stat.st_size
        analysis["date_modified"] = datetime.fromtimestamp(stat.st_mtime).isoformat()
        analysis["md5_hash"] = calculate_md5(trades_file)
        bundle["files"]["trades"] = analysis
    else:
        print(f"   âš ï¸ {trades_file} Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½")
        bundle["integrity_errors"].append({"file": "trades", "error": "File not found", "severity": "critical"})
    
    # 4. Orders CSV
    print("\n4ï¸âƒ£ Ð¤Ð°Ð¹Ð»Ñ‹ Ð¾Ñ€Ð´ÐµÑ€Ð¾Ð²...")
    # Ð˜Ñ‰ÐµÐ¼ Ð² Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾Ð¹ Ð¿Ð°Ð¿ÐºÐµ, Ð·Ð°Ñ‚ÐµÐ¼ Ð² Ð°Ñ€Ñ…Ð¸Ð²Ðµ, Ð·Ð°Ñ‚ÐµÐ¼ Ð² Ñ€Ð°ÑÐ¿Ð°ÐºÐ¾Ð²Ð°Ð½Ð½Ð¾Ð¼ Ð°Ñ€Ñ…Ð¸Ð²Ðµ
    orders_files = list(Path("logs/futures").glob("orders_*.csv"))
    if not orders_files:
        orders_files = list(Path("logs/futures/archived/logs_2025-12-07_16-03-39").glob("orders_*.csv"))
    if not orders_files:
        orders_files = list(Path("logs/futures/archived/logs_2025-12-07_16-03-39_extracted").glob("orders_*.csv"))
    if orders_files:
        latest_orders = max(orders_files, key=lambda p: p.stat().st_mtime)
        print(f"   âœ… {latest_orders}")
        stat = latest_orders.stat()
        bundle["files"]["orders"] = {
            "status": "ok",
            "file_path": str(latest_orders),
            "file_size_bytes": stat.st_size,
            "date_modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
            "md5_hash": calculate_md5(latest_orders)
        }
    else:
        print("   âš ï¸ orders_*.csv Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½")
        bundle["files"]["orders"] = {"status": "missing", "reason": "File not found"}
    
    # 5. Positions Open CSV
    print("\n5ï¸âƒ£ Ð¤Ð°Ð¹Ð»Ñ‹ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¹...")
    positions_files = list(Path("logs/futures").glob("positions_open_*.csv"))
    if not positions_files:
        positions_files = list(Path("logs/futures/archived/logs_2025-12-07_16-03-39").glob("positions_open_*.csv"))
    if not positions_files:
        positions_files = list(Path("logs/futures/archived/logs_2025-12-07_16-03-39_extracted").glob("positions_open_*.csv"))
    if positions_files:
        latest_positions = max(positions_files, key=lambda p: p.stat().st_mtime)
        print(f"   âœ… {latest_positions}")
        stat = latest_positions.stat()
        bundle["files"]["positions_open"] = {
            "status": "ok",
            "file_path": str(latest_positions),
            "file_size_bytes": stat.st_size,
            "date_modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
            "md5_hash": calculate_md5(latest_positions)
        }
    else:
        print("   âš ï¸ positions_open_*.csv Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½")
        bundle["files"]["positions_open"] = {"status": "missing", "reason": "File not found"}
    
    # 6. Signals CSV
    print("\n6ï¸âƒ£ Ð¤Ð°Ð¹Ð»Ñ‹ ÑÐ¸Ð³Ð½Ð°Ð»Ð¾Ð²...")
    signals_files = list(Path("logs/futures").glob("signals_*.csv"))
    if not signals_files:
        signals_files = list(Path("logs/futures/archived/logs_2025-12-07_16-03-39").glob("signals_*.csv"))
    if not signals_files:
        signals_files = list(Path("logs/futures/archived/logs_2025-12-07_16-03-39_extracted").glob("signals_*.csv"))
    if signals_files:
        latest_signals = max(signals_files, key=lambda p: p.stat().st_mtime)
        print(f"   âœ… {latest_signals}")
        stat = latest_signals.stat()
        bundle["files"]["signals"] = {
            "status": "ok",
            "file_path": str(latest_signals),
            "file_size_bytes": stat.st_size,
            "date_modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
            "md5_hash": calculate_md5(latest_signals)
        }
    else:
        print("   âš ï¸ signals_*.csv Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½")
        bundle["files"]["signals"] = {"status": "missing", "reason": "File not found"}
    
    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚
    output_file = f"audit_bundle_{datetime.now().strftime('%Y%m%d')}_futures_scalping.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(bundle, f, indent=2, ensure_ascii=False)
    
    print(f"\n{'=' * 70}")
    print(f"âœ… Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½ Ð²: {output_file}")
    print(f"\nðŸ“Š Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°:")
    ok_files = len([f for f in bundle['files'].values() if isinstance(f, dict) and f.get('status') == 'ok'])
    missing_files = len([f for f in bundle['files'].values() if isinstance(f, dict) and f.get('status') == 'missing'])
    errors = len(bundle['integrity_errors'])
    print(f"   âœ… Ð¤Ð°Ð¹Ð»Ð¾Ð² Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾: {ok_files}")
    print(f"   âš ï¸ Ð¤Ð°Ð¹Ð»Ð¾Ð² Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: {missing_files}")
    print(f"   âŒ ÐžÑˆÐ¸Ð±Ð¾Ðº Ñ†ÐµÐ»Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸: {errors}")
    
    if bundle.get('files', {}).get('trades', {}).get('total_trades'):
        print(f"\nðŸ“ˆ Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° ÑÐ´ÐµÐ»Ð¾Ðº:")
        trades = bundle['files']['trades']
        print(f"   Ð’ÑÐµÐ³Ð¾ ÑÐ´ÐµÐ»Ð¾Ðº: {trades.get('total_trades', 0)}")
        print(f"   ÐŸÑ€Ð¸Ð±Ñ‹Ð»ÑŒÐ½Ñ‹Ñ…: {trades.get('winning_trades', 0)}")
        print(f"   Ð£Ð±Ñ‹Ñ‚Ð¾Ñ‡Ð½Ñ‹Ñ…: {trades.get('losing_trades', 0)}")
        print(f"   ÐžÐ±Ñ‰Ð¸Ð¹ PnL: {trades.get('total_pnl', 0):.4f} USDT")
    
    print(f"{'=' * 70}\n")
    
    return bundle

if __name__ == "__main__":
    main()

